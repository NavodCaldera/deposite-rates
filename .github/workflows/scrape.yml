name: Scrape FD Rates

on:
  # 1. Run on a schedule (this is 22:00 UTC, which is 3:30 AM Sri Lanka Time)
  schedule:
    - cron: '0 22 * * *'
  
  # 2. Allow running manually from the "Actions" tab
  workflow_dispatch:

jobs:
  scrape-and-update:
    # Use a standard Ubuntu (Linux) server
    runs-on: ubuntu-latest
    
    steps:
      # Step 1: Download your code (like 'git pull')
      - name: 1. Check out repository
        uses: actions/checkout@v4

      # Step 2: Set up Python & Cache Dependencies
      - name: 2. Set up Python & Cache Dependencies
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          # This optimization speeds up the process
          cache: 'pip'
          cache-dependency-path: 'scraper/requirements.txt'
          
      # Step 3: Install Python dependencies
      - name: 3. Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r scraper/requirements.txt
          
      # Step 4: Install Playwright browsers
      - name: 4. Install Playwright browsers
        run: |
          # This is a critical step for your async scrapers
          python -m playwright install --with-deps
          
      # Step 5: Run the new scraper orchestrator
      - name: 5. Run the scraper as a module
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
        run: |
          # This command runs your script as a module, which fixes the import errors
          python -m scraper.run_all